{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11071273,"sourceType":"datasetVersion","datasetId":6899261}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bharath4504/major-project?scriptVersionId=237257122\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:14.575462Z","iopub.execute_input":"2025-04-04T02:57:14.575754Z","iopub.status.idle":"2025-04-04T02:57:24.275683Z","shell.execute_reply.started":"2025-04-04T02:57:14.575722Z","shell.execute_reply":"2025-04-04T02:57:24.275011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.27646Z","iopub.execute_input":"2025-04-04T02:57:24.276859Z","iopub.status.idle":"2025-04-04T02:57:24.280565Z","shell.execute_reply.started":"2025-04-04T02:57:24.276835Z","shell.execute_reply":"2025-04-04T02:57:24.279717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1> Dataset Loading</h1>","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/paddy-disease-classification/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.281549Z","iopub.execute_input":"2025-04-04T02:57:24.281966Z","iopub.status.idle":"2025-04-04T02:57:24.332187Z","shell.execute_reply.started":"2025-04-04T02:57:24.28193Z","shell.execute_reply":"2025-04-04T02:57:24.331383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.334128Z","iopub.execute_input":"2025-04-04T02:57:24.334328Z","iopub.status.idle":"2025-04-04T02:57:24.358847Z","shell.execute_reply.started":"2025-04-04T02:57:24.334311Z","shell.execute_reply":"2025-04-04T02:57:24.358073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.360229Z","iopub.execute_input":"2025-04-04T02:57:24.360502Z","iopub.status.idle":"2025-04-04T02:57:24.364929Z","shell.execute_reply.started":"2025-04-04T02:57:24.360481Z","shell.execute_reply":"2025-04-04T02:57:24.364276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define image transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Required for ViT\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.365575Z","iopub.execute_input":"2025-04-04T02:57:24.365796Z","iopub.status.idle":"2025-04-04T02:57:24.382227Z","shell.execute_reply.started":"2025-04-04T02:57:24.365778Z","shell.execute_reply":"2025-04-04T02:57:24.381661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a label mapping (convert class names to numbers)\nlabel_mapping = {label: idx for idx, label in enumerate(train_df['label'].unique())}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.38294Z","iopub.execute_input":"2025-04-04T02:57:24.383223Z","iopub.status.idle":"2025-04-04T02:57:24.404332Z","shell.execute_reply.started":"2025-04-04T02:57:24.383194Z","shell.execute_reply":"2025-04-04T02:57:24.403658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Custom Dataset Class\nclass PaddyDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = self.dataframe.iloc[idx][\"image_id\"]  # Keep original filename\n        class_name = self.dataframe.iloc[idx][\"label\"]   # Get the class label\n        img_path = os.path.join(self.image_dir, class_name, img_name)  # Corrected path\n    \n        # Debug: Print file path\n        if not os.path.exists(img_path):\n            print(f\"File not found: {img_path}\")\n    \n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n    \n        label = label_mapping[class_name]  # Use label mapping for encoding\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.40508Z","iopub.execute_input":"2025-04-04T02:57:24.405348Z","iopub.status.idle":"2025-04-04T02:57:24.420323Z","shell.execute_reply.started":"2025-04-04T02:57:24.405321Z","shell.execute_reply":"2025-04-04T02:57:24.41974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define dataset paths\ntrain_images_path = \"/kaggle/input/paddy-disease-classification/train_images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.421105Z","iopub.execute_input":"2025-04-04T02:57:24.421289Z","iopub.status.idle":"2025-04-04T02:57:24.437176Z","shell.execute_reply.started":"2025-04-04T02:57:24.421273Z","shell.execute_reply":"2025-04-04T02:57:24.436576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-validation-test split (70% train, 20% val, 10% test)\ntrain_df, test_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=2/9, stratify=train_df['label'], random_state=42) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.438046Z","iopub.execute_input":"2025-04-04T02:57:24.438284Z","iopub.status.idle":"2025-04-04T02:57:24.481319Z","shell.execute_reply.started":"2025-04-04T02:57:24.438253Z","shell.execute_reply":"2025-04-04T02:57:24.480672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create dataset instances\ntrain_dataset = PaddyDataset(train_df, train_images_path, transform=transform)\nval_dataset = PaddyDataset(val_df, train_images_path, transform=transform)\ntest_dataset = PaddyDataset(test_df, train_images_path, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.482008Z","iopub.execute_input":"2025-04-04T02:57:24.482195Z","iopub.status.idle":"2025-04-04T02:57:24.486172Z","shell.execute_reply.started":"2025-04-04T02:57:24.482178Z","shell.execute_reply":"2025-04-04T02:57:24.485406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.487024Z","iopub.execute_input":"2025-04-04T02:57:24.48725Z","iopub.status.idle":"2025-04-04T02:57:24.50484Z","shell.execute_reply.started":"2025-04-04T02:57:24.487231Z","shell.execute_reply":"2025-04-04T02:57:24.504074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if DataLoader is working\nimages, labels = next(iter(train_loader))\nprint(f\"Batch Image Shape: {images.shape}, Labels: {labels[:10]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:24.507643Z","iopub.execute_input":"2025-04-04T02:57:24.507861Z","iopub.status.idle":"2025-04-04T02:57:25.128673Z","shell.execute_reply.started":"2025-04-04T02:57:24.507842Z","shell.execute_reply":"2025-04-04T02:57:25.12785Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Vision Transformers</h1>","metadata":{}},{"cell_type":"markdown","source":"<h1>Model Building</h1>","metadata":{}},{"cell_type":"code","source":"from transformers import ViTForImageClassification\nimport torch\n\n# Load Pretrained Vision Transformer Model\nmodel = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\",\n    num_labels=10,  # 10 classes\n    ignore_mismatched_sizes=True  # Fixes the size mismatch error\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:25.13032Z","iopub.execute_input":"2025-04-04T02:57:25.130545Z","iopub.status.idle":"2025-04-04T02:57:47.045304Z","shell.execute_reply.started":"2025-04-04T02:57:25.130526Z","shell.execute_reply":"2025-04-04T02:57:47.044656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Move Model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n# Print Model Summary\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:47.046302Z","iopub.execute_input":"2025-04-04T02:57:47.047056Z","iopub.status.idle":"2025-04-04T02:57:47.504512Z","shell.execute_reply.started":"2025-04-04T02:57:47.047028Z","shell.execute_reply":"2025-04-04T02:57:47.503674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Training</h1>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n# Define Hyperparameters\nbatch_size = 32\nepochs = 5\nlearning_rate = 2e-5  # Standard for fine-tuning transformers\n\n# Define Loss Function & Optimizer\ncriterion = nn.CrossEntropyLoss()  # For multi-class classification\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:47.505336Z","iopub.execute_input":"2025-04-04T02:57:47.505622Z","iopub.status.idle":"2025-04-04T02:57:47.51135Z","shell.execute_reply.started":"2025-04-04T02:57:47.505571Z","shell.execute_reply":"2025-04-04T02:57:47.510473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:47.512141Z","iopub.execute_input":"2025-04-04T02:57:47.512417Z","iopub.status.idle":"2025-04-04T02:57:47.534525Z","shell.execute_reply.started":"2025-04-04T02:57:47.512391Z","shell.execute_reply":"2025-04-04T02:57:47.533674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loss_history = []\nval_loss_history = []\ntrain_acc_history = []\nval_acc_history = []\n\nbest_acc = 0.0  # Track best model accuracy\n\nscaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    total_train_loss, correct_train, total_train = 0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Mixed Precision Training\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            if hasattr(outputs, \"logits\"):  # Ensure compatibility with ViT\n                outputs = outputs.logits\n            loss = criterion(outputs, labels)\n\n        # Backpropagation with AMP\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_train_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct_train += (preds == labels).sum().item()\n        total_train += labels.size(0)\n\n    # Compute Train Loss & Accuracy\n    train_loss = total_train_loss / len(train_loader)\n    train_acc = correct_train / total_train\n    train_loss_history.append(train_loss)\n    train_acc_history.append(train_acc)\n\n    # Validation Loop\n    model.eval()\n    total_val_loss, correct_val, total_val = 0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                if hasattr(outputs, \"logits\"):\n                    outputs = outputs.logits\n                loss = criterion(outputs, labels)\n\n            total_val_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct_val += (preds == labels).sum().item()\n            total_val += labels.size(0)\n\n    # Compute Validation Loss & Accuracy\n    val_loss = total_val_loss / len(val_loader)\n    val_acc = correct_val / total_val\n    val_loss_history.append(val_loss)\n    val_acc_history.append(val_acc)\n\n    # Save Best Model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), \"best_model.pth\")\n\n    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\nprint(\"Training Complete âœ…\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T02:57:47.535456Z","iopub.execute_input":"2025-04-04T02:57:47.535791Z","iopub.status.idle":"2025-04-04T03:21:01.712433Z","shell.execute_reply.started":"2025-04-04T02:57:47.535754Z","shell.execute_reply":"2025-04-04T03:21:01.711665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1> Plots</h1>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Define tick intervals\nx_ticks = np.arange(1, epochs + 1, 1)  # X-axis (Epochs) gap of 1\n# y_loss_ticks = np.arange(0, max(cnn_train_losses + cnn_val_losses) + 0.1, 0.1)  # Y-axis (Loss) gap of 0.1\n# y_acc_ticks = np.arange(0.7, 1.1, 0.1)  # Y-axis (Accuracy) gap of 0.1 (assuming accuracy is 0-1)\n\n# Plot Training & Validation Accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, epochs + 1), train_acc_history, marker='o', color='red', label='Train Accuracy')\nplt.plot(range(1, epochs + 1), val_acc_history, marker='o', color='blue', label='Validation Accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ViT Training & Validation Accuracy Over Epochs\")\n# plt.xticks(x_ticks)  # Set X-axis ticks\n# plt.yticks(y_acc_ticks)  # Set Y-axis ticks\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot Training & Validation Loss\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, epochs + 1), train_loss_history, marker='o', color='red', label='Train Loss')\nplt.plot(range(1, epochs + 1), val_loss_history, marker='o', color='blue', label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"ViT Training & Validation Loss Over Epochs\")\n# plt.xticks(x_ticks)  # Set X-axis ticks\n# plt.yticks(y_loss_ticks)  # Set Y-axis ticks\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:01.713388Z","iopub.execute_input":"2025-04-04T03:21:01.71374Z","iopub.status.idle":"2025-04-04T03:21:02.3054Z","shell.execute_reply.started":"2025-04-04T03:21:01.713706Z","shell.execute_reply":"2025-04-04T03:21:02.304622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Evaluation</h1>","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Lists to store test loss and accuracy\ntest_losses = []\ntest_accuracies = []\n\ncriterion = nn.CrossEntropyLoss()\n\n# List to store predictions\npredictions = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n\n        # Extract logits if using a Vision Transformer\n        if hasattr(outputs, \"logits\"):\n            outputs = outputs.logits\n\n        loss = criterion(outputs, labels)\n        test_losses.append(loss.item())  # Store batch loss\n\n        _, predicted = torch.max(outputs, 1)\n        predictions.extend(predicted.cpu().numpy())\n\n        # Calculate accuracy for this batch\n        batch_accuracy = (predicted == labels).sum().item() / labels.size(0)\n        test_accuracies.append(batch_accuracy)  # Store batch accuracy\n\n# Compute overall Test Loss & Accuracy\navg_test_loss = sum(test_losses) / len(test_losses)\navg_test_accuracy = sum(test_accuracies) / len(test_accuracies)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:02.306259Z","iopub.execute_input":"2025-04-04T03:21:02.306612Z","iopub.status.idle":"2025-04-04T03:21:26.582694Z","shell.execute_reply.started":"2025-04-04T03:21:02.306553Z","shell.execute_reply":"2025-04-04T03:21:26.581943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test Loss: {avg_test_loss:.4f}\\nTest Accuracy: {avg_test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:26.583439Z","iopub.execute_input":"2025-04-04T03:21:26.583701Z","iopub.status.idle":"2025-04-04T03:21:26.588296Z","shell.execute_reply.started":"2025-04-04T03:21:26.583672Z","shell.execute_reply":"2025-04-04T03:21:26.587316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Collect true labels from the test loader\ntrue_labels = []\nfor images, labels in test_loader:\n    true_labels.extend(labels.numpy())\n\n# Generate Classification Report\nreport = classification_report(true_labels, predictions)\nprint(\"Classification Report:\\n\")\nprint(report)\n\n# Generate Confusion Matrix\ncm = confusion_matrix(true_labels, predictions)\n\n# Plotting the Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=np.unique(true_labels), \n            yticklabels=np.unique(true_labels))\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:26.589059Z","iopub.execute_input":"2025-04-04T03:21:26.589313Z","iopub.status.idle":"2025-04-04T03:21:34.594533Z","shell.execute_reply.started":"2025-04-04T03:21:26.589286Z","shell.execute_reply":"2025-04-04T03:21:34.593702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>CNN</h1>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        \n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Assuming input image size is 224x224\n        self.fc2 = nn.Linear(512, num_classes)\n        \n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        \n        x = torch.flatten(x, start_dim=1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Instantiate the CNN model\nnum_classes = len(label_mapping)  # Ensure this matches the dataset\ncnn_model = CNNModel(num_classes)\n\nprint(cnn_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:34.595287Z","iopub.execute_input":"2025-04-04T03:21:34.59581Z","iopub.status.idle":"2025-04-04T03:21:35.028944Z","shell.execute_reply.started":"2025-04-04T03:21:34.595785Z","shell.execute_reply":"2025-04-04T03:21:35.028136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast, GradScaler  # For Mixed Precision Training\n\n# Define device\ncnn_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to device\ncnn_model = cnn_model.to(cnn_device)\n\n# Define Loss Function and Optimizer\ncnn_criterion = nn.CrossEntropyLoss()\ncnn_optimizer = optim.AdamW(cnn_model.parameters(), lr=0.001, weight_decay=1e-4)  # AdamW is more stable\ncnn_scaler = GradScaler()  # For mixed precision\n\n# Training Loop\ncnn_num_epochs = 10  \ncnn_train_losses = []\ncnn_train_accuracies = []\ncnn_val_losses = []\ncnn_val_accuracies = []\n\nfor cnn_epoch in range(cnn_num_epochs):\n    cnn_model.train()\n    cnn_running_loss = 0.0\n    cnn_correct_train = 0\n    cnn_total_train = 0\n\n    for cnn_images, cnn_labels in train_loader:\n        cnn_images, cnn_labels = cnn_images.to(cnn_device, non_blocking=True), cnn_labels.to(cnn_device, non_blocking=True)\n\n        cnn_optimizer.zero_grad()\n\n        # Mixed precision training\n        with autocast():\n            cnn_outputs = cnn_model(cnn_images)\n            cnn_loss = cnn_criterion(cnn_outputs, cnn_labels)\n\n        cnn_scaler.scale(cnn_loss).backward()\n        cnn_scaler.step(cnn_optimizer)\n        cnn_scaler.update()\n\n        cnn_running_loss += cnn_loss.item()\n\n        _, cnn_predicted_train = torch.max(cnn_outputs, 1)\n        cnn_correct_train += (cnn_predicted_train == cnn_labels).sum().item()\n        cnn_total_train += cnn_labels.size(0)\n\n    cnn_avg_train_loss = cnn_running_loss / len(train_loader)\n    cnn_train_losses.append(cnn_avg_train_loss)\n    cnn_train_accuracy = cnn_correct_train / cnn_total_train\n    cnn_train_accuracies.append(cnn_train_accuracy)\n\n    # Validation Phase\n    cnn_model.eval()\n    cnn_val_loss = 0.0\n    cnn_correct_val = 0\n    cnn_total_val = 0\n\n    with torch.no_grad():\n        for cnn_images, cnn_labels in val_loader:\n            cnn_images, cnn_labels = cnn_images.to(cnn_device, non_blocking=True), cnn_labels.to(cnn_device, non_blocking=True)\n\n            with autocast():  # Use mixed precision in validation too\n                cnn_outputs = cnn_model(cnn_images)\n                cnn_loss = cnn_criterion(cnn_outputs, cnn_labels)\n\n            cnn_val_loss += cnn_loss.item()\n\n            _, cnn_predicted_val = torch.max(cnn_outputs, 1)\n            cnn_correct_val += (cnn_predicted_val == cnn_labels).sum().item()\n            cnn_total_val += cnn_labels.size(0)\n\n    cnn_avg_val_loss = cnn_val_loss / len(val_loader)\n    cnn_val_losses.append(cnn_avg_val_loss)\n    cnn_val_accuracy = cnn_correct_val / cnn_total_val\n    cnn_val_accuracies.append(cnn_val_accuracy)\n\n    print(f\"Epoch [{cnn_epoch+1}/{cnn_num_epochs}]: \" \n          f\" Train Accuracy: {cnn_train_accuracy:.4f}, \"\n          f\" Train Loss: {cnn_avg_train_loss:.4f}, \"\n          f\" Val Accuracy: {cnn_val_accuracy:.4f}, \"\n          f\" Val Loss: {cnn_avg_val_loss:.4f} \")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:21:35.029846Z","iopub.execute_input":"2025-04-04T03:21:35.030083Z","iopub.status.idle":"2025-04-04T03:34:56.436286Z","shell.execute_reply.started":"2025-04-04T03:21:35.030065Z","shell.execute_reply":"2025-04-04T03:34:56.435302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Define tick intervals\nx_ticks = np.arange(1, cnn_num_epochs + 1, 1)  # X-axis (Epochs) gap of 1\n# y_loss_ticks = np.arange(0, max(cnn_train_losses + cnn_val_losses) + 0.1, 0.1)  # Y-axis (Loss) gap of 0.1\n# y_acc_ticks = np.arange(0.7, 1.1, 0.1)  # Y-axis (Accuracy) gap of 0.1 (assuming accuracy is 0-1)\n\n# Plot Training & Validation Accuracy\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, cnn_num_epochs + 1), cnn_train_accuracies, marker='o', color='red', label='Train Accuracy')\nplt.plot(range(1, cnn_num_epochs + 1), cnn_val_accuracies, marker='o', color='blue', label='Validation Accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN Training & Validation Accuracy Over Epochs\")\n# plt.xticks(x_ticks)  # Set X-axis ticks\n# plt.yticks(y_acc_ticks)  # Set Y-axis ticks\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot Training & Validation Loss\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, cnn_num_epochs + 1), cnn_train_losses, marker='o', color='red', label='Train Loss')\nplt.plot(range(1, cnn_num_epochs + 1), cnn_val_losses, marker='o', color='blue', label='Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN Training & Validation Loss Over Epochs\")\n# plt.xticks(x_ticks)  # Set X-axis ticks\n# plt.yticks(y_loss_ticks)  # Set Y-axis ticks\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:34:56.437443Z","iopub.execute_input":"2025-04-04T03:34:56.437809Z","iopub.status.idle":"2025-04-04T03:34:56.820886Z","shell.execute_reply.started":"2025-04-04T03:34:56.437772Z","shell.execute_reply":"2025-04-04T03:34:56.820015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set model to evaluation mode\ncnn_model.eval()\n\n# Initialize test loss and accuracy tracking\ncnn_total_test_loss = 0\ncnn_correct_test = 0\ncnn_total_test = 0\n\ncnn_test_losses = []\ncnn_test_accuracies = []\n\ncriterion = nn.CrossEntropyLoss()  # Ensure it matches the training criterion\n\nwith torch.no_grad():\n    for cnn_images, cnn_labels in test_loader:\n        cnn_images, cnn_labels = cnn_images.to(cnn_device, non_blocking=True), cnn_labels.to(cnn_device, non_blocking=True)\n\n        # Forward pass\n        with autocast():  # Use mixed precision for efficiency\n            cnn_outputs = cnn_model(cnn_images)\n            cnn_loss = criterion(cnn_outputs, cnn_labels)\n\n        # Accumulate loss and correct predictions\n        cnn_total_test_loss += cnn_loss.item()\n        _, cnn_predicted_test = torch.max(cnn_outputs, 1)\n\n        cnn_correct_test += (cnn_predicted_test == cnn_labels).sum().item()\n        cnn_total_test += cnn_labels.size(0)\n\n# Compute Test Loss & Accuracy\ncnn_avg_test_loss = cnn_total_test_loss / len(test_loader)\ncnn_test_losses.append(cnn_avg_test_loss)\n\ncnn_test_accuracy = cnn_correct_test / cnn_total_test\ncnn_test_accuracies.append(cnn_test_accuracy)\n\nprint(f\"Test Loss: {cnn_avg_test_loss:.4f}, Test Accuracy: {cnn_test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:34:56.821825Z","iopub.execute_input":"2025-04-04T03:34:56.822164Z","iopub.status.idle":"2025-04-04T03:35:05.155801Z","shell.execute_reply.started":"2025-04-04T03:34:56.822131Z","shell.execute_reply":"2025-04-04T03:35:05.154999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Initialize lists to store all true labels and predictions\nall_true_labels = []\nall_predictions = []\n\n# Iterate over the test set and collect all true labels and predictions\nwith torch.no_grad():\n    for cnn_images, cnn_labels in test_loader:\n        cnn_images, cnn_labels = cnn_images.to(cnn_device, non_blocking=True), cnn_labels.to(cnn_device, non_blocking=True)\n\n        # Forward pass\n        with autocast():  # Use mixed precision for efficiency\n            cnn_outputs = cnn_model(cnn_images)\n            _, cnn_predicted = torch.max(cnn_outputs, 1)\n\n        # Collect true labels and predictions\n        all_true_labels.extend(cnn_labels.cpu().numpy())\n        all_predictions.extend(cnn_predicted.cpu().numpy())\n\n# Now, generate the Classification Report\nCNNreport = classification_report(all_true_labels, all_predictions)\nprint(\"Classification Report:\\n\")\nprint(CNNreport)\n\n# Generate Confusion Matrix\nCNNcm = confusion_matrix(all_true_labels, all_predictions)\n\n# Plotting the Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=np.unique(all_true_labels), \n            yticklabels=np.unique(all_true_labels))\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:35:05.156572Z","iopub.execute_input":"2025-04-04T03:35:05.156907Z","iopub.status.idle":"2025-04-04T03:35:12.514926Z","shell.execute_reply.started":"2025-04-04T03:35:05.15688Z","shell.execute_reply":"2025-04-04T03:35:12.514044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1> Comparitive Analysis</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Plot Training & Validation Accuracy Comparison\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, cnn_num_epochs + 1), cnn_train_accuracies, marker='o', color='red', linestyle='-', label='CNN Train Acc')\n#plt.plot(range(1, cnn_num_epochs + 1), cnn_val_accuracies, marker='o', color='blue', linestyle='--', label='CNN Val Acc')\nplt.plot(range(1, len(train_acc_history) + 1), train_acc_history, marker='s', color='orange', linestyle='-', label='ViT Train Acc')\n#plt.plot(range(1, len(val_acc_history) + 1), val_acc_history, marker='s', color='green', linestyle='--', label='ViT Val Acc')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ViT vs CNN Training Accuracy Comparison\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(8, 5))\n#plt.plot(range(1, cnn_num_epochs + 1), cnn_train_accuracies, marker='o', color='red', linestyle='-', label='CNN Train Acc')\nplt.plot(range(1, cnn_num_epochs + 1), cnn_val_accuracies, marker='o', color='blue', linestyle='--', label='CNN Val Acc')\n#plt.plot(range(1, len(train_acc_history) + 1), train_acc_history, marker='s', color='orange', linestyle='-', label='ViT Train Acc')\nplt.plot(range(1, len(val_acc_history) + 1), val_acc_history, marker='s', color='green', linestyle='--', label='ViT Val Acc')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ViT vs CNN Validation Accuracy Comparison\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n# Plot Training & Validation Loss Comparison\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, cnn_num_epochs + 1), cnn_train_losses, marker='o', color='red', linestyle='-', label='CNN Train Loss')\n#plt.plot(range(1, cnn_num_epochs + 1), cnn_val_losses, marker='o', color='blue', linestyle='--', label='CNN Val Loss')\nplt.plot(range(1, len(train_loss_history) + 1), train_loss_history, marker='s', color='orange', linestyle='-', label='ViT Train Loss')\n#plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, marker='s', color='green', linestyle='--', label='ViT Val Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"ViT vs CNN Training Loss Comparison\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(8, 5))\n#plt.plot(range(1, cnn_num_epochs + 1), cnn_train_losses, marker='o', color='red', linestyle='-', label='CNN Train Loss')\nplt.plot(range(1, cnn_num_epochs + 1), cnn_val_losses, marker='o', color='blue', linestyle='--', label='CNN Val Loss')\n#plt.plot(range(1, len(train_loss_history) + 1), train_loss_history, marker='s', color='orange', linestyle='-', label='ViT Train Loss')\nplt.plot(range(1, len(val_loss_history) + 1), val_loss_history, marker='s', color='green', linestyle='--', label='ViT Val Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"ViT vs CNN Validation Loss Comparison\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:40:20.810798Z","iopub.execute_input":"2025-04-04T03:40:20.811083Z","iopub.status.idle":"2025-04-04T03:40:21.559163Z","shell.execute_reply.started":"2025-04-04T03:40:20.811061Z","shell.execute_reply":"2025-04-04T03:40:21.558443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Test Metrics for CNN & ViT\ntest_metrics = [\"Test Loss\", \"Test Accuracy\"]\ncnn_test_values = [cnn_avg_test_loss, cnn_test_accuracy]\nvit_test_values = [avg_test_loss, avg_test_accuracy]\n\n# X-axis positions\nx = np.arange(len(test_metrics))\n\n# Bar Width\nwidth = 0.35  \n\n# Plot Test Loss & Accuracy Comparison\nplt.figure(figsize=(14, 8))\nplt.bar(x - width/2, cnn_test_values, width, color='red', label=\"CNN\")\nplt.bar(x + width/2, vit_test_values, width, color='blue', label=\"ViT\")\n\n# Labels & Titles\nplt.xlabel(\"Metrics\")\nplt.ylabel(\"Values\")\nplt.title(\"ViT vs CNN Test Performance Comparison\")\nplt.xticks(x, test_metrics)\nplt.ylim(0, 1)  # Normalize scale for comparison\nplt.legend()\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display values on top of bars\nfor i, v in enumerate(cnn_test_values):\n    plt.text(i - width/2, v + 0.02, f\"{v:.4f}\", ha='center', fontsize=12)\nfor i, v in enumerate(vit_test_values):\n    plt.text(i + width/2, v + 0.02, f\"{v:.4f}\", ha='center', fontsize=12)\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:35:13.267388Z","iopub.execute_input":"2025-04-04T03:35:13.267714Z","iopub.status.idle":"2025-04-04T03:35:13.455165Z","shell.execute_reply.started":"2025-04-04T03:35:13.26769Z","shell.execute_reply":"2025-04-04T03:35:13.454491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 7))\nplt.bar([\"CNN\"], [cnn_test_accuracy], color='red', width=0.4, label=\"CNN Test Accuracy\")\nplt.bar([\"ViT\"], [avg_test_accuracy], color='blue', width=0.4, label=\"ViT Test Accuracy\")\n\n# Labels & Titles\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN vs ViT Test Accuracy Comparison\")\nplt.ylim(0, 1)  # Normalize scale\nplt.legend()\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display values on bars\nplt.text(\"CNN\", cnn_test_accuracy + 0.02, f\"{cnn_test_accuracy:.4f}\", ha='center', fontsize=12, color=\"red\")\nplt.text(\"ViT\", avg_test_accuracy + 0.02, f\"{avg_test_accuracy:.4f}\", ha='center', fontsize=12, color=\"blue\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:35:13.455907Z","iopub.execute_input":"2025-04-04T03:35:13.456147Z","iopub.status.idle":"2025-04-04T03:35:13.615688Z","shell.execute_reply.started":"2025-04-04T03:35:13.456115Z","shell.execute_reply":"2025-04-04T03:35:13.61504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 7))\nplt.bar([\"CNN\"], [cnn_avg_test_loss], color='red', width=0.4, label=\"CNN Test Loss\")\nplt.bar([\"ViT\"], [avg_test_loss], color='blue', width=0.4, label=\"ViT Test Loss\")\n\n# Labels & Titles\nplt.ylabel(\"Loss\")\nplt.title(\"CNN vs ViT Test Loss Comparison\")\nplt.ylim(0, 1.3)  # Normalize scale\nplt.legend()\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display values on bars\nplt.text(\"CNN\", cnn_avg_test_loss + 0.02, f\"{cnn_avg_test_loss:.4f}\", ha='center', fontsize=12, color=\"red\")\nplt.text(\"ViT\", avg_test_loss + 0.02, f\"{avg_test_loss:.4f}\", ha='center', fontsize=12, color=\"blue\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T03:35:13.616401Z","iopub.execute_input":"2025-04-04T03:35:13.616631Z","iopub.status.idle":"2025-04-04T03:35:13.78012Z","shell.execute_reply.started":"2025-04-04T03:35:13.6166Z","shell.execute_reply":"2025-04-04T03:35:13.779382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}